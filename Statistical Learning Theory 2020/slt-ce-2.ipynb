{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLT-CE-2: Deterministic Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legi 16-352-137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### References\n",
    "\n",
    "<ol>\n",
    "<li> Deterministic annealing for clustering, compression, classification, regression, and related optimization\n",
    "problems, Kenneth Rose, 1998, http://ieeexplore.ieee.org/document/726788/\n",
    "</li>\n",
    "    \n",
    "<li>\n",
    "A Ratio Scale Metric and the Compatibility\n",
    "of Ratio Scales: The Possibility of\n",
    "Arrowâ€™s Impossibility Theorem, T.L. Saalty, 1994, https://www.sciencedirect.com/science/article/pii/0893965994900930\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "The wine data set, http://www3.dsi.uminho.pt/pcortez/wine5.pdf\n",
    "</li>\n",
    "    \n",
    "<li>\n",
    "Lecture 4, slide 19, https://ml2.inf.ethz.ch/courses/slt/lectures/slt20_lecture04.pdf\n",
    "</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.svm as svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure to install treelib in the slt-ce conda environment: conda install treelib\n",
    "import treelib as tl\n",
    "\n",
    "from sklearn.utils.validation import check_is_fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section Preliminary\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 0.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Implement the function read_X_y_from_csv according to the contract in its docstring.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_X_y_from_csv(sheet, y_names=None):\n",
    "    \"\"\"Parse a column data store into X, y arrays\n",
    "\n",
    "    Args:\n",
    "        sheet (str): Path to csv data sheet.\n",
    "        y_names (list of str): List of column names used as labels.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Array with feature values from columns that are not contained in y_names (n_samples, n_features)\n",
    "        y (dict of np.ndarray): Dictionary with keys y_names, each key contains an array (n_samples, 1)\n",
    "                                with the label data from the corresponding column in sheet. \n",
    "    \"\"\"\n",
    "\n",
    "    # Your code goes here\n",
    "    df = pd.read_csv(sheet)\n",
    "    y = dict({})\n",
    "    for name in y_names:\n",
    "        y[name] = df[name].values\n",
    "        df = df.drop(name, axis=1)\n",
    "    X = df.values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "Read the wine data [3], which contains 11 physiochemical attributes, and two labels (quality and color).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_X_y_from_csv(\"wine-data.csv\", y_names=[\"quality\", \"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 4.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Read reference [1] about deterministic annealing clustering (DAC). Shortly summarize what they refer to as the <i>preferred implementation</i> of the DAC algorithm.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Preferred implementation </i> is a mass-constrained version of deterministic annealing which peculiarity is in its independance on the initialization of clusters-codevectors. We need only to choose maximal number of clusters possible to appear, and then through decreasing the temperature  the algorithm  will increase number of clusters until minimum temperature is achieved. At every step all obtained clusters are checked for splitting, and if they split, new are added (algorithm procedure in detail is on the page 9 of [1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Implement the <b>fit method</b> for the template class DeterministicAnnealing, according to the contract outlined in its docstring.\n",
    "    You can add more class methods as necessary.\n",
    "    See http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html for complementary information.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n",
      "[[7.21282900e+00 3.37145606e-01 3.16607665e-01 5.44209635e+00\n",
      "  5.37195629e-02 3.05122364e+01 1.15741188e+02 9.92387831e-01\n",
      "  3.21612590e+00 5.28910266e-01 1.04898923e+01]]\n",
      "(1, 11)\n",
      "[7.21282900e+00 3.37145606e-01 3.16607665e-01 5.44209635e+00\n",
      " 5.37195629e-02 3.05122364e+01 1.15741188e+02 9.92387831e-01\n",
      " 3.21612590e+00 5.28910266e-01 1.04898923e+01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2550.33477645, 1930.23954174, 1446.4602644 , ...,  615.72010606,\n",
       "         615.72010606,  615.72010606],\n",
       "       [1930.23954174, 1517.22724889, 1097.58655237, ...,  462.89754306,\n",
       "         462.89754306,  462.89754306],\n",
       "       [1446.4602644 , 1097.58655237,  828.04498576, ...,  357.25380042,\n",
       "         357.25380042,  357.25380042],\n",
       "       ...,\n",
       "       [ 615.72010606,  462.89754306,  357.25380042, ...,  159.11137506,\n",
       "         159.11137506,  159.11137506],\n",
       "       [ 615.72010606,  462.89754306,  357.25380042, ...,  159.11137506,\n",
       "         159.11137506,  159.11137506],\n",
       "       [ 615.72010606,  462.89754306,  357.25380042, ...,  159.11137506,\n",
       "         159.11137506,  159.11137506]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(np.mean(X, axis=0)))\n",
    "a = np.empty((1,11))\n",
    "a[0,:] = np.mean(X, axis=0)\n",
    "print(a)\n",
    "print(np.shape(a))\n",
    "print(np.mean(X, axis=0))\n",
    "np.sum(np.ones((np.shape(X)[1],1)), axis=1)\n",
    "np.shape(X)\n",
    "np.cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicAnnealingClustering(skl.base.BaseEstimator, skl.base.TransformerMixin):\n",
    "    \"\"\"Template class for DAC\n",
    "    \n",
    "    Attributes:\n",
    "        cluster_centers_ (np.ndarray): Cluster centroids y_i (n_clusters, n_features)\n",
    "        cluster_probabs_ (np.ndarray): Assignment probability vectors p(y_i | x) for each sample\n",
    "                                       (n_samples, n_clusters)\n",
    "        bifurcation_tree_ (treelib.Tree): Tree object that contains information about cluster evolution during\n",
    "                                          annealing.\n",
    "                                       \n",
    "    Parameters:\n",
    "        n_clusters (int): Maximum number of clusters returned by DAC.\n",
    "        random_state (int): Random seed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=8, random_state=42, metric=\"euclidian\",\\\n",
    "                 Tmin = 0.1, alpha=0.95):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.metric = metric\n",
    "        # Add more parameters, if necessary.\n",
    "        self.current_n_clusters = 1\n",
    "        self.cluster_probabs_ = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.bifurcation_tree_ = None\n",
    "        self.y_prob = None # Marginal probabilities of clusters\n",
    "        self.Tmin = Tmin\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute DAC for input vectors X\n",
    "        \n",
    "        Preferred implementation of DAC as described in reference [1].\n",
    "        Consider to use initialization and reseeding as in sklearn k-means for improved performance.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        \n",
    "        Tmin_flag = False\n",
    "        converged_flag = False\n",
    "        N = np.shape(X)[0] # Number of samples\n",
    "        F = np.shape(X)[1] # Number of features\n",
    "        while not (Tmin_flag):\n",
    "            if self.current_n_clusters == 1:\n",
    "                C_x = np.cov(X)\n",
    "                eig = np.linalg.eigvals(C_x)\n",
    "                ab_eig = np.absolute(eig)\n",
    "                lambda_max = np.max(ab_eig)\n",
    "                lambda_max = np.max(np.abs(np.linalg.eigvals(C_x)))\n",
    "                T = 3*lambda_max\n",
    "                self.cluster_centers_ = np.empty((1,F))\n",
    "                self.cluster_centers_[0, :] = np.mean(X, axis=0)\n",
    "                self.cluster_probabs_ = np.ones((F,1))\n",
    "                self.y_prob = np.sum(self.cluster_probabs_, axis=0)\n",
    "                \n",
    "            converged_flag = False    \n",
    "            while not converged_flag:\n",
    "                if self.metric == \"euclidian\":\n",
    "\n",
    "                    # Your code goes here\n",
    "\n",
    "                    # update p(y|x)\n",
    "                    for i in range(self.current_n_clusters):\n",
    "                        self.cluster_probabs_[i, :] = self.y_prob[i] * \\\n",
    "                            np.exp(-1.0/T*np.linalg.norm(X - self.cluster_centers_[i, :])**2)\n",
    "                    for i in range(self.current_n_clusters): \n",
    "                        self.cluster_probabs_[:, i] /= sum(self.cluster_probabs_[:, i]) \n",
    "\n",
    "                    # update p(y)    \n",
    "                    self.y_prob = np.sum(self.cluster_probabs_, axis=0) / N\n",
    "\n",
    "                    # update y\n",
    "                    cluster_centers_new = X @ self.cluster_probabs_ / self.y_prob / N\n",
    "                    \n",
    "                    # Convergence check\n",
    "                    if np.linalg.norm(cluster_centers_new - self.cluster_centers_) < 1e-5:\n",
    "                        converged_flag = True\n",
    "                    self.cluster_centers_ = cluster_centers_new\n",
    "                        \n",
    "                #elif metric == \"ratioscale\":\n",
    "                    # code for extension\n",
    "                    \n",
    "                # Temperature check and cooling step\n",
    "                if T <= self.Tmin:\n",
    "                    T = 1e-9\n",
    "                    Tmin_flag = True\n",
    "                else:\n",
    "                    T *= self.alpha\n",
    "                print(T)\n",
    "                \n",
    "                if self.current_n_clusters < self.n_clusters:\n",
    "                    for i in range(self.current_n_clusters):\n",
    "                        # Calculate covariance matrix of posterior\n",
    "                        C = np.zeros(F)\n",
    "                        for j in range(N):\n",
    "                            D = (X[j,:] - self.cluster_centers_[i, :]) \\\n",
    "                            @ np.transpose(X[j,:] - self.cluster_centers_[i, :])\n",
    "                            C += 1.0 / N * self.cluster_probabs_[j, i] * D\n",
    "                        \n",
    "                        # Add cluster depending on covariance and temperature\n",
    "                        l_max = max(np.abs(np.linalg.eigvals(np.cov(C))))\n",
    "                        if T < 2*l_max:\n",
    "                            print(self.current_n_clusters)\n",
    "                            self.current_n_clusters += 1\n",
    "                            self.cluster_centers_ = np.vstack(self.cluster_centers_,\\\n",
    "                                                             self.cluster_centers_[i,:])\n",
    "                            a = np.random.rand(np.shape(self.cluster_centers_[0,:]))\n",
    "                            self.cluster_centers[self.current_n_clusters-1, :] += a\n",
    "                            self.y_prob[:, i] /= 2.0\n",
    "                            self.y_prob = np.hstack(self.y_prob, self.y_prob[:, i])                            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict assignment probability vectors for each sample in X.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (new_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            P (np.ndarray): Assignment probability vectors (new_samples, n_clusters) \n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        \n",
    "        return P\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X to a cluster-distance space.\n",
    "        \n",
    "        In the new space, each dimension is the distance to the cluster centers. \n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input array with shape (new_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Y (np.ndarray): Cluster-distance vectors (new_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"cluster_centers_\"])\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        \n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def plot_bifurcation(self):\n",
    "        \"\"\"Show the evolution of cluster splitting\"\"\"\n",
    "        check_is_fitted(self, [\"bifurcation_tree_\"])\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Create an instance of your DAC class with n_clusters = 2 and <b>fit the first 6000 samples</b> of the wine data set. Record the execution time. Furthermore, create an instance of the sklearn k-means class, and fit it with the same parameters. Again record the execution time.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/anaconda3/envs/slt-ce/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                        y[\"color\"],\n",
    "                                                                        train_size=6000,\n",
    "                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DAC = DeterministicAnnealingClustering(n_clusters=2, random_state=42)\n",
    "DAC.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 7.97 ms, total: 243 ms\n",
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = cluster.KMeans(n_clusters=2,random_state=42)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.5\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 4.5 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    <ul style=\"background-color:#adebad;\">\n",
    "        <li> \n",
    "            Implement the <b>predict method</b> for the template class DAC, according to the contract outlined in its docstring.\n",
    "        </li>\n",
    "        <li>\n",
    "            Use DAC.predict and kmeans.predict to predict the cluster labels of X_test.\n",
    "        </li>\n",
    "        <li>\n",
    "            Compute the confusion matrix between the two predictions as described in <br>\n",
    "            http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_kmeans = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_DAC = DAC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "<li> Before we can compute the confusion matrix, we need to perform some post-processing on the DAC cluster assignments.\n",
    "    Explain what the function postprocess (defined below) does, and why we need it. To do so, complete the docstring of the function postprocess.\n",
    "        </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(y_DAC, y_kmeans):\n",
    "    \"\"\"TODO: Add explanation\"\"\"\n",
    "    \n",
    "    y_DAC_hard = np.argmax(y_DAC, axis=1)\n",
    "    \n",
    "    n_clusters = len(np.unique(y_DAC_hard))\n",
    "    dac2kmeans = []\n",
    "    for cluster in range(n_clusters):\n",
    "        argmax = np.argmax(y_DAC[:, cluster])\n",
    "        dac2kmeans.append(y_kmeans[argmax])\n",
    "        \n",
    "    y_DAC_new = []\n",
    "    for dac_label in y_DAC_hard:\n",
    "        y_DAC_new.append(dac2kmeans[dac_label])\n",
    "        \n",
    "    return np.array(y_DAC_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(y_kmeans, postprocess(y_DAC, y_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.0 </span>\n",
    "</h2>\n",
    "\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li> Implement the <b>transform method</b> for the template class DAC, according to the contract outlined in its docstring.\n",
    "        </li>\n",
    "        <li>\n",
    "        Use DAC.transform and kmeans.transform to transform both, X_train and X_test. \n",
    "        </li>\n",
    "       \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_DAC = DAC.transform(X_train)\n",
    "X_test_DAC = DAC.transform(X_test)\n",
    "\n",
    "X_train_kmeans = kmeans.transform(X_train)\n",
    "X_test_kmeans = kmeans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Fit an SVM classifier with default parameters to the untransformed data, and to the transformed data.\n",
    "        Compare the performance of predicting whether the color of a wine is red or white.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_DAC = svm.SVC(random_state=42)\n",
    "svm_DAC.fit(X_train_DAC, y_train)\n",
    "svm_DAC.score(X_test_DAC, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state=42)\n",
    "svm.fit(X_train_kmeans, y_train)\n",
    "svm.score(X_test_kmeans, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Produce two scatter plots, one for X_train_DAC and one for X_train_kmeans.<br>\n",
    "        Make the marker color indicate the wine color.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "        Create a fixed 2D embedding (e.g. with LLE, t-SNE, MDS) of the wine data and color the markers according to quality and color. Fit and transform X_train with DAC(n_clusters=3,4,5,6,7,8,...). Produce a plot of the SVM score svm_DAC.score(X_test_DAC, y_test) as a function of n_clusters.. Each time use marker shapes to display the cluster memberships, and compare to the labels color and quality.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    %%time\n",
    "    lle = skl.manifold.LocallyLinearEmbedding(random_state=...)\n",
    "    lle.fit(...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.5\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.5 </span>\n",
    "</h2>\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "            Produce a phase diagram plot of the expected distortion D, as shown in figure 2 of reference [1]. For this, extend DAC.fit to save the expected distortion during annealing as an additional attribute self.distortion.\n",
    "            You might also want to save the number of effective clusters and the temperature along the way.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# extend DAC.fit(self, X):\n",
    "    # ...\n",
    "    # Save information for each (n-th) annealing step:\n",
    "    # self.distortion = [d0, d1, d2, ...]\n",
    "    # self.n_eff_clusters = [e0, e1, e2, ...]\n",
    "    # self.temp = [t0, t1, t2, ...]\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Implement DAC.plot_bifurcation, which should create a bifurcation plot as shown on slide 19 of lecture 3. As our data is not 1-dimensional as in the lecture slide, we will have to adapt our scheme, so that the distances between nodes of the tree make sense.<br>\n",
    "        Modify DAC.fit to keep track of the distances, using the tree object DAC.bifurcation\\_tree\\_. When a cluster splits, it creates two child nodes. Each node should store its centroid vector, and the distance to the parent centroid vector. After splitting, the parent node is not updated anymore.<br>\n",
    "        In the bifurcation plot, the horizontal distance of a child node to its parent node should be exactly the distance to the parent centroid vector. The two child nodes should move in opposite directions, i.e. one to the left of the parent and one to the right.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "        Argue how reasonable our method of plotting the bifurcation is. Explain how the 1D-distances between nodes (i.e. nodes that are not siblings) do not correspond exactly to the distances between centroids. Suggest ideas for improvement.\n",
    "        </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 6.0\n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 6.0 </span>\n",
    "</h2>\n",
    "\n",
    "<ul style=\"background-color:#adebad;\">\n",
    "        <li>\n",
    "            So far, our implementation of DAC assumed that our data is compatible with the euclidian metric. Argue why this assumption is not justified for the wine-data.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "        All the features of the wine-data set are measured on a ratio scale, which is incompatible with the euclidian metric (Remark: this is not the complete answer to problem 6, argue why they are not compatible). A more appropriate distance is proposed in reference [2]:\n",
    "            <br><br>\n",
    "            $d(x,y)=\\log{ \\frac{1}{d^2} \\sum_{i,j=1}^d \\frac{x_i}{x_j} \\frac{y_j}{y_i}}$\n",
    "            <br><br>\n",
    "            Extend DAC.fit to the case of metric == ratioscale, using d(x,y) as given above.<br>\n",
    "            Hint 1: As this distance does not give a closed form update formula for the centroids $y$, you will need to do gradient descent to update the centroids. You can either calculate the gradient by hand, or use an automatic differentiation tool like Tensorflow. If you calculate the gradient by hand, provide the formula in Latex below.\n",
    "                    <br>\n",
    "            Hint 2: Keep in mind the possibility of negative definite matrices and appropriate regularization when solving for the critical temperature of the ratio scale approach.\n",
    "        </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#adebad;\">\n",
    "    <li>\n",
    "    Perform experiments to compare the euclidian and ratioscale metrics.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#4286f4;\"> Comments </h2>\n",
    "\n",
    "Let us know what you liked about this exercise, and what we can improve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
